{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "TCu7p0fecnId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def hist_dist(title, distribution_tensor, hist_range=(-4, 4)):\n",
        "    \"\"\"\n",
        "    Display histogram of a TF distribution\n",
        "    \"\"\"\n",
        "    with tf.Session() as sess:\n",
        "        values = sess.run(distribution_tensor)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.hist(values, np.linspace(*hist_range, num=len(values)/2))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def _get_loss_acc(dataset, weights):\n",
        "    \"\"\"\n",
        "    Get losses and validation accuracy of example neural network\n",
        "    \"\"\"\n",
        "    batch_size = 128\n",
        "    epochs = 2\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    features = tf.placeholder(tf.float32)\n",
        "    labels = tf.placeholder(tf.float32)\n",
        "    learn_rate = tf.placeholder(tf.float32)\n",
        "\n",
        "    biases = [\n",
        "        tf.Variable(tf.zeros([256])),\n",
        "        tf.Variable(tf.zeros([128])),\n",
        "        tf.Variable(tf.zeros([dataset.train.labels.shape[1]]))\n",
        "    ]\n",
        "\n",
        "    # Layers\n",
        "    layer_1 = tf.nn.relu(tf.matmul(features, weights[0]) + biases[0])\n",
        "    layer_2 = tf.nn.relu(tf.matmul(layer_1, weights[1]) + biases[1])\n",
        "    logits = tf.matmul(layer_2, weights[2]) + biases[2]\n",
        "\n",
        "    # Training loss\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
        "\n",
        "    # Accuracy\n",
        "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    # Measurements use for graphing loss\n",
        "    loss_batch = []\n",
        "\n",
        "    with tf.Session() as session:\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        batch_count = int((dataset.train.num_examples / batch_size))\n",
        "\n",
        "        # The training cycle\n",
        "        for epoch_i in range(epochs):\n",
        "            for batch_i in range(batch_count):\n",
        "                batch_features, batch_labels = dataset.train.next_batch(batch_size)\n",
        "\n",
        "                # Run optimizer and get loss\n",
        "                session.run(\n",
        "                    optimizer,\n",
        "                    feed_dict={features: batch_features, labels: batch_labels, learn_rate: learning_rate})\n",
        "                l = session.run(\n",
        "                    loss,\n",
        "                    feed_dict={features: batch_features, labels: batch_labels, learn_rate: learning_rate})\n",
        "                loss_batch.append(l)\n",
        "\n",
        "        valid_acc = session.run(\n",
        "            accuracy,\n",
        "            feed_dict={features: dataset.validation.images, labels: dataset.validation.labels, learn_rate: 1.0})\n",
        "\n",
        "    # Hack to Reset batches\n",
        "    dataset.train._index_in_epoch = 0\n",
        "    dataset.train._epochs_completed = 0\n",
        "\n",
        "    return loss_batch, valid_acc\n",
        "\n",
        "\n",
        "def compare_init_weights(\n",
        "        dataset,\n",
        "        title,\n",
        "        weight_init_list,\n",
        "        plot_n_batches=100):\n",
        "    \"\"\"\n",
        "    Plot loss and print stats of weights using an example neural network\n",
        "    \"\"\"\n",
        "    colors = ['r', 'b', 'g', 'c', 'y', 'k']\n",
        "    label_accs = []\n",
        "    label_loss = []\n",
        "\n",
        "    assert len(weight_init_list) <= len(colors), 'Too many inital weights to plot'\n",
        "\n",
        "    for i, (weights, label) in enumerate(weight_init_list):\n",
        "        loss, val_acc = _get_loss_acc(dataset, weights)\n",
        "\n",
        "        plt.plot(loss[:plot_n_batches], colors[i], label=label)\n",
        "        label_accs.append((label, val_acc))\n",
        "        label_loss.append((label, loss[-1]))\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Batches')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "    print('After 858 Batches (2 Epochs):')\n",
        "    print('Validation Accuracy')\n",
        "    for label, val_acc in label_accs:\n",
        "        print('  {:7.3f}% -- {}'.format(val_acc*100, label))\n",
        "    print('Loss')\n",
        "    for label, loss in label_loss:\n",
        "        print('  {:7.3f}  -- {}'.format(loss, label))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}